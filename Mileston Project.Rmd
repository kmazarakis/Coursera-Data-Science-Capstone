---
title: "Mileston Project"
author: "Kleanthis Mazarakis"
date: "January 5, 2018"
output: html_document
---

#Introduction
The Coursera Data Science Capstone involves predictive text analytics. The overall objective is to help users complete sentences by analyzing the words they have entered and predicating the next word. 

The purpose of this Milestone Report is to demonstrate progress towards the end goal of this project. The specific sections are as follows:

- Load/Profile Raw Data - demonstrate progress with loading the data into R & profiling the raw data
- Create Sample Corpus - prepare the data prior to NLP
- Create N-grams to Explore Data - create n-grams & explore the word patterns
- Next Steps - discuss plans for creating the prediction algorighm & shiny application

#Load the Data and do some summary Statistics


```{r}

library(stringi)
library(tm)
library(RWeka)
library(ggplot2)

twitter <- readLines("en_US.twitter.txt")
news <- readLines("en_US.news.txt")
blogs <- readLines("en_US.blogs.txt")

```

In the next step we are counting the number of lines and the number of words in each of our datasets

``` {r}
#Number of lines
paste("We have ", length(twitter), "lines in the twitter dataset")
paste("We have ", length(news), "lines in the news dataset")
paste("We have ", length(blogs), "lines in the blogs dataset")

#Number of words
paste("We have ", sum(stri_count_words(twitter)), "words in the twitter dataset")
paste("We have ", sum(stri_count_words(news)), "words in the news dataset")
paste("We have ", sum(stri_count_words(blogs)), "words in the blogs dataset")
```

In the next step we are going to randomly sample our datasets in order to get a smaller dataset that we can work with


#Create Sample dataset and Corpus file

```{r}
#Sample lines from each file
twitter_sample <- sample(twitter, 1000) #length(twitter)*0.01)
news_sample <- sample(news, 1000) #length(news)*0.01)
blogs_sample <- sample(blogs, 1000) #length(blogs)*0.01)

length(twitter_sample)
length(news_sample)
length(blogs_sample)

alldata_sample <- c(twitter_sample, news_sample, blogs_sample)

corpus_sample <- VCorpus(VectorSource(alldata_sample))
#Placeholder for potentially removing stopwods at later stage
##corpus_sample <- tm_map(corpus_sample, removeWords, stopwords("en"))
corpus_sample <- tm_map(corpus_sample, removeNumbers)
corpus_sample <- tm_map(corpus_sample, stripWhitespace)
corpus_sample <- tm_map(corpus_sample, content_transformer(tolower))
corpus_sample <- tm_map(corpus_sample, removePunctuation)
text_corpus <- data.frame(text = unlist(sapply(corpus_sample, `[`, "content")), stringsAsFactors = F)
text_corpus[1:5,]
```

#Data Exploration with Ngrams

```{r}
unigrams <- NGramTokenizer(text_corpus, Weka_control(min=1, max=1))
bigrams <- NGramTokenizer(text_corpus, Weka_control(min=2, max=2))
trigrams <- NGramTokenizer(text_corpus, Weka_control(min=3, max=3))
quadgrams <- NGramTokenizer(text_corpus, Weka_control(min=4, max=4))

unigrams_table <- data.frame(table(unigrams))
bigrams_table <- data.frame(table(bigrams))
trigrams_table <- data.frame(table(trigrams))
quadgrams_table <- data.frame(table(quadgrams))

unigrams_table_top20 <- head(unigrams_table[order(unigrams_table$Freq, decreasing = T),],20)
bigrams_table_top20 <- head(bigrams_table[order(bigrams_table$Freq, decreasing = T),],20)
trigrams_table_top20 <- head(trigrams_table[order(trigrams_table$Freq, decreasing = T),],20)
quadgrams_table_top20 <- head(quadgrams_table[order(quadgrams_table$Freq, decreasing = T),],20)

unigrams_table_top20
bigrams_table_top20
trigrams_table_top20
quadgrams_table_top20
```

###Top 20 Unigrams
```{r}
ggplot(unigrams_table_top20, aes(x=unigrams, y=Freq)) + geom_bar(stat = "Identity") + geom_text(aes(label=Freq)) + theme(axis.text.x = element_text(angle = 45))
```


###Top 20 Bigrams
```{r}
ggplot(bigrams_table_top20, aes(x=bigrams, y=Freq)) + geom_bar(stat = "Identity") + geom_text(aes(label=Freq)) + theme(axis.text.x = element_text(angle = 45))
```

###Top 20 Trigrams
```{r}
ggplot(trigrams_table_top20, aes(x=trigrams, y=Freq)) + geom_bar(stat = "Identity") + geom_text(aes(label=Freq)) + theme(axis.text.x = element_text(angle = 45))

```


###Top 20 Quadgrams
```{r}
ggplot(quadgrams_table_top20, aes(x=quadgrams, y=Freq)) + geom_bar(stat = "Identity") + geom_text(aes(label=Freq)) + theme(axis.text.x = element_text(angle = 45))

```

#Next Steps
The next step of this project will be to build our predicting algorithm and deploy it as a Shiny application.
Our strategy for building the algorithm will potentially be to use the trigrams created in the above analysis in order to predict the next word. If there is no match for the trigram, then the algorithm would fall back to using the bigrams and the unigrams as a last resort.
In the final step of the project we will also need to take account of the time required for the prediction, since our model should be responsive in "real time".